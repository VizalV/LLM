{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e291b18d-a1f0-4068-9290-c9b937567e3e",
      "metadata": {
        "id": "e291b18d-a1f0-4068-9290-c9b937567e3e"
      },
      "source": [
        "# Cracking Open the OpenAI API"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f25acaea-c334-4254-8635-64270dc6c397",
      "metadata": {
        "id": "f25acaea-c334-4254-8635-64270dc6c397"
      },
      "source": [
        "### set-up"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio -q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "npq_lhqSGq-a",
        "outputId": "e0b1bd7a-fc39-40e7-a174-61fdbfaab3ef"
      },
      "id": "npq_lhqSGq-a",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m54.0/54.0 MB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m322.6/322.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m95.2/95.2 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.5/11.5 MB\u001b[0m \u001b[31m76.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.5/62.5 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1edd17a3-6cde-4cdf-8f29-c9e1e533d390",
      "metadata": {
        "id": "1edd17a3-6cde-4cdf-8f29-c9e1e533d390"
      },
      "outputs": [],
      "source": [
        "from openai import OpenAI\n",
        "import openai\n",
        "import time\n",
        "import gradio as gr"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1c75b3c4-cce9-4d82-888c-078a4e9ae1fa",
      "metadata": {
        "id": "1c75b3c4-cce9-4d82-888c-078a4e9ae1fa"
      },
      "source": [
        "### code"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "100350ea-c690-49d3-84cc-84b192c18500",
      "metadata": {
        "id": "100350ea-c690-49d3-84cc-84b192c18500"
      },
      "source": [
        "##### First call"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eb7a1c4f-b4dc-453f-8e63-bc016f2de35e",
      "metadata": {
        "id": "eb7a1c4f-b4dc-453f-8e63-bc016f2de35e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f822c260-aa3e-44be-cc78-b9b06c14dcc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OpenAI API key set: True\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "openai.api_key = userdata.get('OPENAI_API_KEY')\n",
        "os.environ['OPENAI_API_KEY'] = openai.api_key\n",
        "\n",
        "# Verify that the key is set\n",
        "print(f\"OpenAI API key set: {bool(openai.api_key)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# create a chat completion\n",
        "completion = openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"developer\", \"content\": \"Talk like a pirate.\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"How do I check if a Python object is an instance of a class?\",\n",
        "        },\n",
        "    ],\n",
        ")\n",
        "completion.choices[0].message.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "id": "Di5DuVK0dZ-9",
        "outputId": "80591d0b-50b5-4bd0-f9d7-35a13422f7e2"
      },
      "id": "Di5DuVK0dZ-9",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Arrr, matey! If ye be wantin\\' to check if a Python object be an instance of a particular class, ye can use the `isinstance()` function. Here be how ye do it:\\n\\n```python\\nclass Pirate:\\n    pass\\n\\njack = Pirate()\\n\\n# Check if \\'jack\\' be an instance of \\'Pirate\\'\\nif isinstance(jack, Pirate):\\n    print(\"Aye, this scallywag be a Pirate!\")\\nelse:\\n    print(\"Nay, this landlubber be not a Pirate!\")\\n```\\n\\nThis \\'ere `isinstance()` will tell ye true or false if yer object be a member of the class ye be checkin\\' against. So set sail and give it a try, savvy? Yarrr!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "completion = openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Complete the sentence\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Jack and \",\n",
        "        },\n",
        "    ],\n",
        ")"
      ],
      "metadata": {
        "id": "k5Im1MzSInFF"
      },
      "id": "k5Im1MzSInFF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6288eb8-03d7-45f5-9e40-3912afe2a39c",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6288eb8-03d7-45f5-9e40-3912afe2a39c",
        "outputId": "97380a8e-b471-4bf3-a402-ec75952e0d64"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'id': 'chatcmpl-BR2HYC7d4fk1OVf1NWJv1qQFHyule',\n",
              " 'choices': [{'finish_reason': 'stop',\n",
              "   'index': 0,\n",
              "   'logprobs': None,\n",
              "   'message': {'content': 'Jill went up the hill to fetch a pail of water.',\n",
              "    'refusal': None,\n",
              "    'role': 'assistant',\n",
              "    'annotations': []}}],\n",
              " 'created': 1745783328,\n",
              " 'model': 'gpt-4o-mini-2024-07-18',\n",
              " 'object': 'chat.completion',\n",
              " 'service_tier': 'default',\n",
              " 'system_fingerprint': 'fp_129a36352a',\n",
              " 'usage': {'completion_tokens': 15,\n",
              "  'prompt_tokens': 17,\n",
              "  'total_tokens': 32,\n",
              "  'completion_tokens_details': {'accepted_prediction_tokens': 0,\n",
              "   'audio_tokens': 0,\n",
              "   'reasoning_tokens': 0,\n",
              "   'rejected_prediction_tokens': 0},\n",
              "  'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}}"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "completion.to_dict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "340f7ccf-c599-443c-89d8-e509039d673b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "340f7ccf-c599-443c-89d8-e509039d673b",
        "outputId": "cacda3e1-eefe-4250-94b5-37a6a3451c8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jill went up the hill to fetch a pail of water.\n"
          ]
        }
      ],
      "source": [
        "# print the chat completion\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9b0e3b38-0bb2-48de-9d3f-ee5013f4a689",
      "metadata": {
        "tags": [],
        "id": "9b0e3b38-0bb2-48de-9d3f-ee5013f4a689"
      },
      "source": [
        "##### max tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf02f54e-004e-409b-942d-74a4d855c2d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf02f54e-004e-409b-942d-74a4d855c2d5",
        "outputId": "107827a3-bbbe-425e-f06e-f6d1600f7388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jill\n"
          ]
        }
      ],
      "source": [
        "# create a chat completion\n",
        "completion = openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Complete the sentence\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Jack and \",\n",
        "        },\n",
        "    ],\n",
        "    max_tokens = 2\n",
        ")\n",
        "\n",
        "# print the chat completion\n",
        "print(completion.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "800f966f-7370-46d3-ae12-c955565b9617",
      "metadata": {
        "id": "800f966f-7370-46d3-ae12-c955565b9617"
      },
      "source": [
        "##### n = number of chat completions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ad49735c-88c2-4563-a1c6-0020a8905e8e",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ad49735c-88c2-4563-a1c6-0020a8905e8e",
        "outputId": "6b3d9539-b6e5-4cd2-e403-feca7e54560e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jill\n",
            "Jill\n"
          ]
        }
      ],
      "source": [
        "# create a chat completion\n",
        "completion = openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Complete the sentence\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Jack and \",\n",
        "        },\n",
        "    ],\n",
        "    max_tokens = 2,\n",
        "    n=2\n",
        ")\n",
        "\n",
        "# print the chat completion\n",
        "for i in range(len(completion.choices)):\n",
        "    print(completion.choices[i].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "183ffa97-eb11-4ca9-a8d4-29502ee0cdc6",
      "metadata": {
        "id": "183ffa97-eb11-4ca9-a8d4-29502ee0cdc6"
      },
      "source": [
        "##### temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "638b828b-b323-4cd8-90ea-5106146635a0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "638b828b-b323-4cd8-90ea-5106146635a0",
        "outputId": "71cb4f84-52d5-41e7-94b1-b6bce7141f3a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Jill went up the hill to fetch a pail of water.\n",
            "Jill went up the hill to fetch a pail of water.\n",
            "Jill went up the hill to fetch a pail of water.\n"
          ]
        }
      ],
      "source": [
        "# create a chat completion\n",
        "completion = openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    messages=[\n",
        "        {\"role\": \"system\", \"content\": \"Complete the sentence\"},\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"Jack and \",\n",
        "        },\n",
        "    ],\n",
        "    # max_tokens = 2,\n",
        "    n=3,\n",
        "    temperature=2\n",
        ")\n",
        "\n",
        "# print the chat completion\n",
        "for i in range(len(completion.choices)):\n",
        "    print(completion.choices[i].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# IMAGES"
      ],
      "metadata": {
        "id": "A0I6yUUOpuSN"
      },
      "id": "A0I6yUUOpuSN"
    },
    {
      "cell_type": "code",
      "source": [
        "import base64\n",
        "\n",
        "# Function to encode the image\n",
        "def encode_image(image_path):\n",
        "    with open(image_path, \"rb\") as image_file:\n",
        "        return base64.b64encode(image_file.read()).decode(\"utf-8\")\n",
        "\n",
        "\n",
        "# Path to your image\n",
        "image_path = \"/content/Screenshot 2025-04-23 164457.png\"\n",
        "\n",
        "# Getting the Base64 string\n",
        "base64_image = encode_image(image_path)\n",
        "\n",
        "\n",
        "response = openai.responses.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "    input=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                { \"type\": \"input_text\", \"text\": \"what's in this image?\" },\n",
        "                {\n",
        "                    \"type\": \"input_image\",\n",
        "                    \"image_url\": f\"data:image/jpeg;base64,{base64_image}\",\n",
        "                },\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        ")\n",
        "\n",
        "print(response.output_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xTdpMPx9ntBe",
        "outputId": "43e3a3be-3a44-446e-8840-cedb3c48b432"
      },
      "id": "xTdpMPx9ntBe",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The image appears to depict a 3D scene featuring two cannon-like objects positioned on a sloped surface, with several black spheres floating above them. The background is mostly sky blue, indicating a clear sky. The design resembles a game or simulation, possibly involving projectile motion or physics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A CHATBOT without the UI"
      ],
      "metadata": {
        "id": "NNZmACJVngUF"
      },
      "id": "NNZmACJVngUF"
    },
    {
      "cell_type": "code",
      "source": [
        "import openai\n",
        "\n",
        "# Initialize the chat messages history\n",
        "messages = [{\"role\": \"assistant\", \"content\": \"How can I help?\"}]\n",
        "\n",
        "# Function to display the chat history\n",
        "def display_chat_history(messages):\n",
        "    for message in messages:\n",
        "        print(f\"{message['role'].capitalize()}: {message['content']}\")\n",
        "\n",
        "# Function to get the assistant's response\n",
        "def get_assistant_response(messages):\n",
        "    r = openai.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[{\"role\": m[\"role\"], \"content\": m[\"content\"]} for m in messages],\n",
        "    )\n",
        "    response = r.choices[0].message.content\n",
        "    return response\n",
        "\n",
        "# Main chat loop\n",
        "while True:\n",
        "    # Display chat history\n",
        "    display_chat_history(messages)\n",
        "\n",
        "    # Get user input\n",
        "    prompt = input(\"User: \")\n",
        "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "    # Get assistant response\n",
        "    response = get_assistant_response(messages)\n",
        "    messages.append({\"role\": \"assistant\", \"content\": response})"
      ],
      "metadata": {
        "id": "4jEg7tPknZ1M",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 841
        },
        "outputId": "7c1c7bf2-f749-476a-c15f-be4fd559f3c7"
      },
      "id": "4jEg7tPknZ1M",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Assistant: How can I help?\n",
            "User: What day comes after Thursday?\n",
            "Assistant: How can I help?\n",
            "User: What day comes after Thursday?\n",
            "Assistant: The day that comes after Thursday is Friday.\n",
            "User: Great, Why is Friday considered amazing?\n",
            "Assistant: How can I help?\n",
            "User: What day comes after Thursday?\n",
            "Assistant: The day that comes after Thursday is Friday.\n",
            "User: Great, Why is Friday considered amazing?\n",
            "Assistant: Friday is often considered amazing for several reasons:\n",
            "\n",
            "1. **End of the Workweek**: For many people, Friday marks the end of the traditional workweek. This often leads to a sense of relief and anticipation for the weekend.\n",
            "\n",
            "2. **Social Activities**: Many people plan social activities on Fridays, such as going out to dinner, attending events, or spending time with friends and family, which can create excitement and enjoyment.\n",
            "\n",
            "3. **Casual Atmosphere**: In some workplaces, Friday is viewed as a more relaxed day, sometimes featuring casual dress codes or team-building activities.\n",
            "\n",
            "4. **Friday Night Traditions**: Many have Friday night routines or traditions, whether it‚Äôs movie nights, game nights, or other leisurely activities that help them unwind.\n",
            "\n",
            "5. **Anticipation of the Weekend**: Friday carries a sense of anticipation, as it's the gateway to Saturday and Sunday, which often allow people more freedom for leisure, hobbies, and rest.\n",
            "\n",
            "6. **Cultural Significance**: In some cultures, Friday holds special significance, such as in Islam where it is considered a holy day. In Western culture, it has developed a positive connotation associated with fun and relaxation.\n",
            "\n",
            "These factors contribute to the general perception of Friday as an \"amazing\" day of the week!\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-89e07e207176>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0;31m# Get user input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"User: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m     \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc228536-a0b6-4a7f-81f7-382c3205ee8a",
      "metadata": {
        "tags": [],
        "id": "cc228536-a0b6-4a7f-81f7-382c3205ee8a"
      },
      "source": [
        "### Demo: Data Science Assistant"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def data_science_chatbot(user_query):\n",
        "    system_prompt = (\n",
        "        \"You are a helpful Data Science teaching assistant. \" #Role\n",
        "        \"You can answer questions related to machine learning, data analysis, statistics, and AI concepts. \" # Domain\n",
        "        \"Explain answers clearly and in simple terms suitable for students.\" # Style\n",
        "    )\n",
        "\n",
        "    response = openai.chat.completions.create(\n",
        "    model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": system_prompt},\n",
        "            {\"role\": \"user\", \"content\": user_query}\n",
        "        ]\n",
        "    )\n",
        "    return response.choices[0].message.content"
      ],
      "metadata": {
        "id": "N41Vjnv-GjKR"
      },
      "id": "N41Vjnv-GjKR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gr.Interface(\n",
        "    fn=data_science_chatbot,\n",
        "    inputs=gr.Textbox(lines=2, placeholder=\"Ask a data science question...\"),\n",
        "    outputs=gr.Textbox(),\n",
        "    title=\"üß† Data Science Assistant Bot\",\n",
        "    description=\"Ask questions about machine learning, data analysis, statistics, and AI concepts.\"\n",
        ").launch(share=True,debug=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "u_o520o7Glw0",
        "outputId": "5f42d5bd-cfb0-414f-be5f-b52ddcd0b74e",
        "collapsed": true
      },
      "id": "u_o520o7Glw0",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://2cfd0c105788edda5e.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2cfd0c105788edda5e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://2cfd0c105788edda5e.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Enter Your Query Here\"\n",
        "\n",
        "def message_and_history(input, history):\n",
        "    history = history or []\n",
        "    s = list(sum(history, ()))\n",
        "    s.append(input)\n",
        "    inp = ' '.join(s)\n",
        "    output = data_science_chatbot(inp)\n",
        "    history.append((input, output))\n",
        "    return history, history\n",
        "\n",
        "\n",
        "block = gr.Blocks(theme=gr.themes.Monochrome())\n",
        "with block:\n",
        "    gr.Markdown(\"\"\"<h1><center>ChatGPT\n",
        "    ChatBot with gr and OpenAI</center></h1>\n",
        "    \"\"\")\n",
        "    chatbot = gr.Chatbot()\n",
        "    message = gr.Textbox(placeholder=prompt)\n",
        "    state = gr.State()\n",
        "    submit = gr.Button(\"SEND\")\n",
        "    submit.click(message_and_history,\n",
        "                 inputs=[message, state],\n",
        "                 outputs=[chatbot, state])\n",
        "block.launch(debug = True,share=True)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 650
        },
        "id": "ceMjehyJL18y",
        "outputId": "00da2a5d-dbc1-4399-f410-ef7cd438273e",
        "collapsed": true
      },
      "id": "ceMjehyJL18y",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-f2aa0f707a3a>:18: UserWarning: You have not specified a value for the `type` parameter. Defaulting to the 'tuples' format for chatbot messages, but this is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style dictionaries with 'role' and 'content' keys.\n",
            "  chatbot = gr.Chatbot()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://200e0cfaddddc316ce.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://200e0cfaddddc316ce.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aB4nGkYZYKbf"
      },
      "id": "aB4nGkYZYKbf",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}