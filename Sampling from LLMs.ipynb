{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "aYoL6zWxbN_G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sampling Text"
      ],
      "metadata": {
        "id": "IWCN4zu9AqVA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VYBfqmRbv2NS"
      },
      "outputs": [],
      "source": [
        "# change this to any decoder only LLM\n",
        "device = 'cuda:0'\n",
        "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-ZqAx9qzDqb"
      },
      "source": [
        "## Greedy Search Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NMtTjm24y_dS"
      },
      "outputs": [],
      "source": [
        "def greedy_decoding(model, text, max_length=10):\n",
        "    # tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=False)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # loop until the maximum length is reached\n",
        "        for _ in range(max_length):\n",
        "            # feed X_{1...t} and get token logits for t+1 th step\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # find the most likely token\n",
        "            next_token_id = torch.argmax(logits, dim=-1, keepdim=True).to(device)\n",
        "\n",
        "            # append X_{t+1} to the input sequence\n",
        "            input_ids = torch.cat((input_ids, next_token_id), dim=-1)\n",
        "\n",
        "            # break if <eos> token is generated\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # decode the generated tokens and return the text\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K1sx1L2ifHFk",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "outputId": "b381a448-fe46-4aee-beaa-77830f34795a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-26de12a332f4>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy_decoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"It rains a lot in the\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"---\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgreedy_decoding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Tell me about apples:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-24-7285c5c7a20f>\u001b[0m in \u001b[0;36mgreedy_decoding\u001b[0;34m(model, text, max_length)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0;31m# find the most likely token\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m             \u001b[0mnext_token_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeepdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0;31m# append X_{t+1} to the input sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m             \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menviron\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"CUDA_MODULE_LOADING\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"LAZY\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
          ]
        }
      ],
      "source": [
        "print(greedy_decoding(model, \"It rains a lot in the\"))\n",
        "print(\"---\")\n",
        "print(greedy_decoding(model, \"Tell me about apples:\", 50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgUsBVmAa5ku"
      },
      "source": [
        "## Decoding with Sampling: Top K"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "482bEmmXSVZK"
      },
      "outputs": [],
      "source": [
        "def sampling_decoding_top_k(model, text, top_k=50, max_length=30):\n",
        "    # tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # loop until the maximum length is reached\n",
        "        for _ in range(max_length):\n",
        "            # feed X_{1...t} and get token logits for t+1 th step\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # find top_k tokens\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            top_k_logits, top_k_indices = sorted_logits[:, :top_k], sorted_indices[:, :top_k]\n",
        "\n",
        "            # redistribute the probability mass using softmax\n",
        "            top_k_probs = torch.softmax(top_k_logits, dim=-1)\n",
        "\n",
        "            # randomly sample a token based on the probability distribution\n",
        "            chosen_idx = torch.multinomial(top_k_probs, num_samples=1).to(device)\n",
        "            next_token_id = top_k_indices.gather(-1, chosen_idx)\n",
        "\n",
        "            # append X_{t+1} to the input sequence\n",
        "            input_ids = torch.cat((input_ids, next_token_id), dim=-1)\n",
        "\n",
        "            # break if <eos> token is generated\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # decode the generated tokens and return the text\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_jZT3WYLRGR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "d19cca31-03ed-4c78-c882-45c69d63d61e"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-b778218f3625>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_decoding_top_k\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"It rains a lot in the\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "print(sampling_decoding_top_k(model, \"It rains a lot in the\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X8iU_HUtbJeE"
      },
      "source": [
        "## Decoding with Sampling: Top P"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xCf7xxqpWCt9"
      },
      "outputs": [],
      "source": [
        "def sampling_decoding_top_p(model, text, top_p=0.92, max_length=30):\n",
        "    # tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # loop until the maximum length is reached\n",
        "        for _ in range(max_length):\n",
        "            # feed X_{1...t} and get token logits for t+1 th step\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # find the minimum set of tokens whose cumulative probability is above the threshold\n",
        "            sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "            sorted_probs = torch.softmax(sorted_logits, dim=-1)\n",
        "\n",
        "            cumulative_prob = torch.cumsum(sorted_probs, dim=-1)\n",
        "            top_p_num = (cumulative_prob > top_p).nonzero(as_tuple=True)[1][0].item() + 1\n",
        "\n",
        "            top_p_logits, top_p_indices = sorted_logits[:, :top_p_num], sorted_indices[:, :top_p_num]\n",
        "\n",
        "            # redistribute the probability mass using softmax\n",
        "            top_p_probs = torch.softmax(top_p_logits, dim=-1)\n",
        "\n",
        "            # randomly sample a token based on the probability distribution\n",
        "            chosen_idx = torch.multinomial(top_p_probs, num_samples=1).to(device)\n",
        "            next_token_id = top_p_indices.gather(-1, chosen_idx)\n",
        "\n",
        "            # append X_{t+1} to the input sequence\n",
        "            input_ids = torch.cat((input_ids, next_token_id), dim=-1)\n",
        "\n",
        "            # break if <eos> token is generated\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # decode the generated tokens and return the text\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBf9itTwTywB",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 146
        },
        "outputId": "fae10893-6f07-4cc8-c464-40169f6f59a4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-6221936e2581>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msampling_decoding_top_p\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"It rains a lot in the\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.82\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ],
      "source": [
        "print(sampling_decoding_top_p(model, \"It rains a lot in the\", 0.82, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJkbsxpFbKon"
      },
      "source": [
        "## Decoding with Sampling: Temperature"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v8Jf-rLGXWVE"
      },
      "outputs": [],
      "source": [
        "def sampling_decoding_temperature(model, text, temperature=1, max_length=30):\n",
        "    # tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # loop until the maximum length is reached\n",
        "        for _ in range(max_length):\n",
        "            # feed X_{1...t} and get token logits for t+1 th step\n",
        "            outputs = model(input_ids)\n",
        "            logits = outputs.logits[:, -1, :]\n",
        "\n",
        "            # apply softmax with temperature\n",
        "            logits = logits / temperature\n",
        "            probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "            # sample from the distribution\n",
        "            next_token_id = torch.multinomial(probs, num_samples=1).to(device)\n",
        "\n",
        "            # append X_{t+1} to the input sequence\n",
        "            input_ids = torch.cat((input_ids, next_token_id), dim=-1)\n",
        "\n",
        "            # break if <eos> token is generated\n",
        "            if next_token_id == tokenizer.eos_token_id:\n",
        "                break\n",
        "\n",
        "    # decode the generated tokens and return the text\n",
        "    generated_text = tokenizer.decode(input_ids[0], skip_special_tokens=False)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWxu2ageYSA1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f67b35cb-bc13-42ec-b7e2-fb795ed3b95a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It rains a lot in the summer, but it's not like it's raining\n",
            "---\n",
            "Tell me about apples: For years, bitter apples have been the hallmark of thru-hikers. In 2002, student Kevin Rosenberg of Modesto spent two weeks biking from Sun Valley to Massachusetts to view maple maple months back at 2:00 in the morning. He wasn't\n"
          ]
        }
      ],
      "source": [
        "print(sampling_decoding_temperature(model, \"It rains a lot in the\", 0.2, 10))\n",
        "print(\"---\")\n",
        "print(sampling_decoding_temperature(model, \"Tell me about apples:\", 1.0, 50))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-kx4MojcS9G"
      },
      "source": [
        "## Beam Search Decoding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iw6NgyCUcbsr"
      },
      "outputs": [],
      "source": [
        "def beam_search_decoding(model, text, num_beams=3, max_length=10):\n",
        "    # tokenize the input text\n",
        "    input_ids = tokenizer.encode(text, return_tensors='pt', add_special_tokens=False).to(device)\n",
        "\n",
        "    # initialize the beams\n",
        "    # list of tuples (token_ids, product of probabilities)\n",
        "    beams = [(input_ids, 1)]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(max_length):\n",
        "            all_candidates = []\n",
        "\n",
        "            for input_ids, prod_prob in beams:\n",
        "                outputs = model(input_ids)\n",
        "                logits = outputs.logits[:, -1, :]\n",
        "\n",
        "                # get the probabilities\n",
        "                probs = torch.softmax(logits, dim=-1)\n",
        "\n",
        "                # select the top num_beams tokens and their probabilities\n",
        "                top_probs, top_token_ids = torch.topk(probs, num_beams, dim=-1)\n",
        "\n",
        "                for i in range(num_beams):\n",
        "                    next_token_id = top_token_ids[0, i].unsqueeze(0).unsqueeze(0).to(device)\n",
        "                    next_prob = top_probs[0, i].item()\n",
        "\n",
        "                    new_input_ids = torch.cat((input_ids, next_token_id), dim=-1)\n",
        "                    new_prod_prob = prod_prob * next_prob\n",
        "\n",
        "                    all_candidates.append((new_input_ids, new_prod_prob))\n",
        "\n",
        "            # keep the top num_beams sequences\n",
        "            beams = sorted(all_candidates, key=lambda x: x[1], reverse=True)[:num_beams]\n",
        "\n",
        "            # break if all sequences in beams end with <eos> token\n",
        "            if all(tokenizer.eos_token_id in beam[0][0] for beam in beams):\n",
        "                break\n",
        "\n",
        "    # decode the best sequence (the one with the highest prod probability)\n",
        "    best_sequence = beams[0][0]\n",
        "    generated_text = tokenizer.decode(best_sequence[0], skip_special_tokens=False)\n",
        "    return generated_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "188R_xVdusBx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7914180-7b13-4df0-be81-c82a34485855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It rains a lot in the summer, so it's a good time to get\n"
          ]
        }
      ],
      "source": [
        "print(beam_search_decoding(model, \"It rains a lot in the\", 3, 10))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_next_token_prediction():\n",
        "    tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
        "    model = AutoModelForCausalLM.from_pretrained('gpt2')\n",
        "\n",
        "    text = \"Hi, my\"\n",
        "    inputs = tokenizer.encode(text, return_tensors='pt')\n",
        "    outputs = model.generate(inputs, max_length=20, num_return_sequences=1)\n",
        "\n",
        "    print(\"Generated Text:\", tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
        "\n",
        "demonstrate_next_token_prediction()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KJ9A1kgUsEBx",
        "outputId": "727c20a1-7538-4a52-af2a-6fbc2ffd9057"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text: Hi, my name is John. I'm a writer and a musician. I'm a musician.\n"
          ]
        }
      ]
    }
  ]
}